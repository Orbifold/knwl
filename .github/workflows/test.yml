name: Test

on:
  push:
    branches: [ "master", "main" ]
  pull_request:
    branches: [ "master", "main" ]

permissions:
  contents: read

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install uv
      uses: astral-sh/setup-uv@v2
      with:
        version: "latest"
    
    - name: Install dependencies
      run: |
        uv sync --dev
    
    - name: Run tests (excluding LLM-dependent tests)
      run: |
        uv run pytest --cov=knwl --cov-report=xml -m "not llm" -v
    
    - name: Upload coverage reports to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.python-version == '3.11'
      env:
        CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  test-llm:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || contains(github.event.head_commit.message, '[test-llm]')
    strategy:
      matrix:
        python-version: ["3.11"]

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install uv
      uses: astral-sh/setup-uv@v2
      with:
        version: "latest"
    
    - name: Install dependencies
      run: |
        uv sync --dev
    
    - name: Setup Ollama (for LLM tests)
      run: |
        curl -fsSL https://ollama.ai/install.sh | sh
        ollama serve &
        sleep 10
        ollama pull llama3.2:1b  # Use a small model for testing
        
    - name: Run LLM tests
      run: |
        uv run pytest -m "llm" -v --tb=short
      env:
        OLLAMA_HOST: "http://localhost:11434"