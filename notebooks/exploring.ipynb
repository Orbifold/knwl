{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "from knwl.base import QueryParam\n",
    "\n",
    "ENCODER = None\n",
    "\n",
    "\n",
    "def encode_string_by_tiktoken(content: str, model_name: str = \"gpt-4o\"):\n",
    "    global ENCODER\n",
    "    if ENCODER is None:\n",
    "        ENCODER = tiktoken.encoding_for_model(model_name)\n",
    "    tokens = ENCODER.encode(content)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def decode_tokens_by_tiktoken(tokens: list[int], model_name: str = \"gpt-4o\"):\n",
    "    global ENCODER\n",
    "    if ENCODER is None:\n",
    "        ENCODER = tiktoken.encoding_for_model(model_name)\n",
    "    content = ENCODER.decode(tokens)\n",
    "    return content\n",
    "\n",
    "\n",
    "def chunking_by_token_size(\n",
    "        content: str, overlap_token_size=128, max_token_size=1024, tiktoken_model=\"gpt-4o\"\n",
    "):\n",
    "    tokens = encode_string_by_tiktoken(content, model_name=tiktoken_model)\n",
    "    results = []\n",
    "    for index, start in enumerate(\n",
    "            range(0, len(tokens), max_token_size - overlap_token_size)\n",
    "    ):\n",
    "        chunk_content = decode_tokens_by_tiktoken(\n",
    "            tokens[start: start + max_token_size], model_name=tiktoken_model\n",
    "        )\n",
    "        results.append(\n",
    "            {\n",
    "                \"tokens\": min(max_token_size, len(tokens) - start),\n",
    "                \"content\": chunk_content.strip(),\n",
    "                \"index\": index,\n",
    "            }\n",
    "        )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./books/relativity.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "toks = chunking_by_token_size(content, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toks[0][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toks[1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging\n",
    "\n",
    "Ah, the logging definition inside the package can't be hijacked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Stuff:\n",
    "    a: int\n",
    "    b: str\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        return self.a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Stuff(a=1654, b=\"hello\")\n",
    "c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Union\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def locate_json_string_body_from_string(content: str) -> Union[str, None]:\n",
    "    \"\"\"Locate the JSON string body from a string\"\"\"\n",
    "    maybe_json_str = re.search(r\"{.*}\", content, re.DOTALL)\n",
    "    if maybe_json_str is not None:\n",
    "        return maybe_json_str.group(0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "locate_json_string_body_from_string(\n",
    "    \"\"\"\n",
    "    A lot of text here\n",
    "    =====\n",
    "\n",
    "    {\n",
    "        \"a\": 1,\n",
    "        \"b\": 2\n",
    "    }\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from hashlib import md5\n",
    "\n",
    "\n",
    "def compute_args_hash(*args):\n",
    "    return md5(str(args).encode()).hexdigest()\n",
    "\n",
    "\n",
    "compute_args_hash(\"hello\", 1, 2, 3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "from knwl.storage import JsonStorage\n",
    "\n",
    "ollama_client = ollama.AsyncClient()\n",
    "messages = []\n",
    "history_messages = []\n",
    "\n",
    "hashing_kv: JsonStorage = JsonStorage()\n",
    "messages.extend(history_messages)\n",
    "messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "if hashing_kv is not None:\n",
    "    args_hash = compute_args_hash(model, messages)\n",
    "    if_cache_return = await hashing_kv.get_by_id(args_hash)\n",
    "    if if_cache_return is not None:\n",
    "        return if_cache_return[\"return\"]\n",
    "\n",
    "response = await ollama_client.chat(model=model, messages=messages, **kwargs)\n",
    "\n",
    "result = response[\"message\"][\"content\"]\n",
    "\n",
    "if hashing_kv is not None:\n",
    "    await hashing_kv.upsert({args_hash: {\"return\": result, \"model\": model}})\n",
    "\n",
    "return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "input = [\"My name is Swa\", \"The weather is nice today\", \"What is my name? (explain your answer)\"]\n",
    "msgs = [{\"role\": \"user\", \"content\": i} for i in input]\n",
    "ollama.chat(\"llama3.2\", msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knwl.utils import split_string_by_multi_markers\n",
    "\n",
    "split_string_by_multi_markers('\"hello\",\"world;this is a test\"', [\",\", \";\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'entities': {'JOHN': [{'description': 'John is a person who knows Maria.', 'entity_name': 'JOHN', 'entity_type': 'PERSON', 'source_id': 'a'}], 'MARIA': [{'description': 'Maria is known by John.', 'entity_name': 'MARIA', 'entity_type': 'PERSON', 'source_id': 'a'}]}, 'relationships': {('JOHN', 'MARIA'): [{'description': 'John and Maria are acquainted with each other.', 'keywords': 'acquaintance, social connection', 'source_id': 'a', 'src_id': 'JOHN', 'tgt_id': 'MARIA', 'weight': 5.0}]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dumps(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knwl.storage import VectorStorage\n",
    "\n",
    "store = VectorStorage(namespace=\"edges\")\n",
    "await store.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await store.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await  store.query(\"sadfs sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knwl.prompt import PROMPTS\n",
    "from knwl.llm import llm\n",
    "q = PROMPTS[\"keywords_extraction\"].format(query=\"John and Maria are acquainted with each other.\")\n",
    "await  llm.ask(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"John and Maria are acquainted with each other.\"\n",
    "keywords_prompt = PROMPTS[\"keywords_extraction\"].format(query=query)\n",
    "result = {'high_level_keywords': ['Acquaintance', 'Relationship'],\n",
    "          'low_level_keywords': ['John', 'Maria', 'Know each other']}\n",
    "result = (\n",
    "    result.replace(keywords_prompt[:-1], \"\")\n",
    "    .replace(\"user\", \"\")\n",
    "    .replace(\"model\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "result = \"{\" + result.split(\"{\")[1].split(\"}\")[0] + \"}\"\n",
    "\n",
    "keywords_data = json.loads(result)\n",
    "low_keywords = keywords_data.get(\"low_level_keywords\", [])\n",
    "low_keywords = \", \".join(low_keywords)\n",
    "low_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T05:33:09.747211Z",
     "start_time": "2024-11-05T05:32:57.459001Z"
    }
   },
   "outputs": [],
   "source": [
    "from knwl.simple import Simple, QueryParam\n",
    "\n",
    "s = Simple()\n",
    "found = await s.query(\"Who is John?\", QueryParam(mode=\"local\"))\n",
    "print(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"entity\"|John|PERSON|\"A person named John who met someone.\")\n",
      "(\"entity\"|Maria|PERSON|\"A person named Maria who met John.\")\n",
      "(\"entity\"|London|LOCATION|\"The city where John and Maria were heading towards during their meeting.\")\n",
      "\n",
      "(\"relationship\"|John|Maria|\"John and Maria met each other.\"|\"Meeting\"|1)\n",
      "\n",
      "(\"content_keywords\"|Meeting, Traveling, London)\n",
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"-Goal-\n",
    "Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n",
    "\n",
    "-Steps-\n",
    "1. Identify all entities. For each identified entity, extract the following information:\n",
    "- entity_name: Name of the entity, capitalized\n",
    "- entity_type: One of the following types: [{entity_types}]\n",
    "- entity_description: Comprehensive description of the entity's attributes and activities\n",
    "Format each entity as (\"entity\"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>\n",
    "\n",
    "2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n",
    "For each pair of related entities, extract the following information:\n",
    "- source_entity: name of the source entity, as identified in step 1\n",
    "- target_entity: name of the target entity, as identified in step 1\n",
    "- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n",
    "- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n",
    "- relationship_keywords: one or more high-level key words that summarize the overarching nature of the relationship, focusing on concepts or themes rather than specific details\n",
    "Format each relationship as (\"relationship\"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_keywords>{tuple_delimiter}<relationship_strength>)\n",
    "\n",
    "3. Identify high-level key words that summarize the main concepts, themes, or topics of the entire text. These should capture the overarching ideas present in the document.\n",
    "Format the content-level key words as (\"content_keywords\"{tuple_delimiter}<high_level_keywords>)\n",
    "\n",
    "4. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter.\n",
    "\n",
    "5. When finished, output {completion_delimiter}\n",
    "######################\n",
    "-Real Data-\n",
    "######################\n",
    "Entity_types: {entity_types}\n",
    "Text: {input_text}\n",
    "######################\n",
    "Output:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = prompt.format(input_text=\"John met Maria on the way to London.\", entity_types=\"PERSON, LOCATION, ORGANIZATION\", record_delimiter=\"|||\", completion_delimiter=\"DONE\", tuple_delimiter=\"|\")\n",
    "\n",
    "import ollama\n",
    "from IPython.display import display, Markdown\n",
    "r = ollama.chat(\"qwen2.5:7b\", [{\"role\": \"user\", \"content\": prompt}])\n",
    "print(r[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"entity\"|John|PERSON|Person named John who interacted with another individual in a specific event.)\n",
      "(\"entity\"|Maria|PERSON|Person named Maria who interacted with another individual in a specific event.)\n",
      "(\"entity\"|London|LOCATION|A city that serves as the destination for a journey mentioned in the text.)\n",
      "(\"relationship\"|John|Maria|John met Maria, indicating an interaction between them.|interaction|1)\n",
      "(\"relationship\"|John|London|John is traveling to London, suggesting a goal or destination.|travel|2)\n",
      "(\"relationship\"|Maria|London|Maria is also on her way to London, implying she has the same goal as John.|travel|2)\n",
      "\n",
      "(\"content_keywords\"|interaction, travel, destination)\n",
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(r[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
